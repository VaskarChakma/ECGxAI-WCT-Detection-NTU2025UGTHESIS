{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X, y, features, df[target].name\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m X, y, features, target_name = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# ==============================================\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# SECTION 3: MODEL DEFINITION AND HYPERPARAMETERS\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# ==============================================\u001b[39;00m\n\u001b[32m     93\u001b[39m \n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Define models with their hyperparameters including ranges and optimal values\u001b[39;00m\n\u001b[32m     95\u001b[39m models = {\n\u001b[32m     96\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mXGBoost\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m: XGBClassifier,\n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m     }\n\u001b[32m    169\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m():\n\u001b[32m     58\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load and preprocess the ECG data.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmerged_ecg_data_cleaned.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# Ensure balanced classes\u001b[39;00m\n\u001b[32m     62\u001b[39m     df = df.groupby(\u001b[33m'\u001b[39m\u001b[33mwct_label_encoded\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mwct_label_encoded\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mwct_label\u001b[39m\u001b[33m'\u001b[39m).head(\u001b[32m50000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, f1_score, confusion_matrix, \n",
    "                           classification_report, roc_auc_score, r2_score,\n",
    "                           mean_squared_error, mean_absolute_error)\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 1: CONFIGURATION AND SETUP (UPDATED)\n",
    "# ==============================================\n",
    "\n",
    "# Set global configurations\n",
    "RANDOM_SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_JOBS = -1  # Use all cores\n",
    "N_SIMULATIONS = 25  # Number of Monte Carlo simulations\n",
    "CV_FOLDS = 10  # 10-fold cross validation\n",
    "\n",
    "# Set matplotlib configurations for high-quality plots with Times New Roman\n",
    "plt.style.use('seaborn-v0_8')  # Updated to use correct seaborn style name\n",
    "mpl.rcParams['font.family'] = 'Times New Roman'\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['axes.titlesize'] = 14\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "mpl.rcParams['xtick.labelsize'] = 10\n",
    "mpl.rcParams['ytick.labelsize'] = 10\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = True  # Add grid by default\n",
    "mpl.rcParams['grid.alpha'] = 0.3  # Make grid lines semi-transparent\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "os.makedirs('output/tables', exist_ok=True)\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 2: DATA LOADING AND PREPROCESSING\n",
    "# ==============================================\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load and preprocess the ECG data.\"\"\"\n",
    "    df = pd.read_csv('merged_ecg_data_cleaned.csv')\n",
    "    \n",
    "    # Ensure balanced classes\n",
    "    df = df.groupby('wct_label_encoded' if 'wct_label_encoded' in df.columns else 'wct_label').head(50000)\n",
    "    \n",
    "    features = ['bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end', \n",
    "               'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', \n",
    "               't_axis', 'qrs_duration']\n",
    "    target = 'wct_label_encoded' if 'wct_label_encoded' in df.columns else 'wct_label'\n",
    "    \n",
    "    # Data cleaning\n",
    "    for col in features:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    df = df.dropna(subset=[target])\n",
    "    \n",
    "    if not np.issubdtype(df[target].dtype, np.number):\n",
    "        le = LabelEncoder()\n",
    "        df[target] = le.fit_transform(df[target])\n",
    "    \n",
    "    X = df[features].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}, Class distribution: {np.bincount(y)}\")\n",
    "    return X, y, features, df[target].name\n",
    "\n",
    "# Load data\n",
    "X, y, features, target_name = load_data()\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 3: MODEL DEFINITION AND HYPERPARAMETERS\n",
    "# ==============================================\n",
    "\n",
    "# Define models with their hyperparameters including ranges and optimal values\n",
    "models = {\n",
    "    \"XGBoost\": {\n",
    "        'model': XGBClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'n_jobs': N_JOBS,\n",
    "            'tree_method': 'hist'\n",
    "        },\n",
    "        'param_ranges': {\n",
    "            'n_estimators': {'range': [100, 500], 'optimal': 200},\n",
    "            'max_depth': {'range': [3, 10], 'optimal': 5},\n",
    "            'learning_rate': {'range': [0.01, 0.2], 'optimal': 0.1},\n",
    "            'subsample': {'range': [0.6, 1.0], 'optimal': 0.8},\n",
    "            'colsample_bytree': {'range': [0.6, 1.0], 'optimal': 0.8}\n",
    "        }\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'n_jobs': N_JOBS\n",
    "        },\n",
    "        'param_ranges': {\n",
    "            'n_estimators': {'range': [100, 500], 'optimal': 200},\n",
    "            'max_depth': {'range': [3, 10], 'optimal': 5},\n",
    "            'learning_rate': {'range': [0.01, 0.2], 'optimal': 0.1},\n",
    "            'subsample': {'range': [0.6, 1.0], 'optimal': 0.8},\n",
    "            'colsample_bytree': {'range': [0.6, 1.0], 'optimal': 0.8}\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 10,\n",
    "            'min_samples_split': 5,\n",
    "            'min_samples_leaf': 2,\n",
    "            'random_state': RANDOM_SEED,\n",
    "            'n_jobs': N_JOBS,\n",
    "            'class_weight': 'balanced'\n",
    "        },\n",
    "        'param_ranges': {\n",
    "            'n_estimators': {'range': [100, 500], 'optimal': 200},\n",
    "            'max_depth': {'range': [5, 20], 'optimal': 10},\n",
    "            'min_samples_split': {'range': [2, 10], 'optimal': 5},\n",
    "            'min_samples_leaf': {'range': [1, 5], 'optimal': 2}\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        'model': GradientBoostingClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'random_state': RANDOM_SEED\n",
    "        },\n",
    "        'param_ranges': {\n",
    "            'n_estimators': {'range': [100, 500], 'optimal': 200},\n",
    "            'max_depth': {'range': [3, 10], 'optimal': 5},\n",
    "            'learning_rate': {'range': [0.01, 0.2], 'optimal': 0.1},\n",
    "            'subsample': {'range': [0.6, 1.0], 'optimal': 0.8}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create detailed hyperparameter table\n",
    "hyperparam_data = []\n",
    "for name, details in models.items():\n",
    "    for param, value in details['params'].items():\n",
    "        if param in details['param_ranges']:\n",
    "            hyperparam_data.append({\n",
    "                'Model': name,\n",
    "                'Parameter': param,\n",
    "                'Optimal Value': value,\n",
    "                'Range Min': details['param_ranges'][param]['range'][0],\n",
    "                'Range Max': details['param_ranges'][param]['range'][1],\n",
    "                'Value Used': value\n",
    "            })\n",
    "        else:  # For parameters without specified ranges (like random_state)\n",
    "            hyperparam_data.append({\n",
    "                'Model': name,\n",
    "                'Parameter': param,\n",
    "                'Optimal Value': value,\n",
    "                'Range Min': 'N/A',\n",
    "                'Range Max': 'N/A',\n",
    "                'Value Used': value\n",
    "            })\n",
    "\n",
    "hyperparam_table = pd.DataFrame(hyperparam_data)\n",
    "\n",
    "# Reorder columns for better readability\n",
    "hyperparam_table = hyperparam_table[['Model', 'Parameter', 'Range Min', 'Range Max', 'Optimal Value', 'Value Used']]\n",
    "\n",
    "# Save hyperparameter table\n",
    "hyperparam_table.to_csv('output/tables/model_hyperparameters_detailed.csv', index=False)\n",
    "print(\"\\nSaved detailed hyperparameter table to output/tables/model_hyperparameters_detailed.csv\")\n",
    "\n",
    "# Create a summary table of just the optimal parameters\n",
    "optimal_params_table = pd.DataFrame([\n",
    "    {\n",
    "        'Model': name,\n",
    "        'Optimal Parameters': ', '.join([f\"{k}={v}\" for k, v in details['params'].items()])\n",
    "    }\n",
    "    for name, details in models.items()\n",
    "])\n",
    "\n",
    "optimal_params_table.to_csv('output/tables/optimal_parameters_summary.csv', index=False)\n",
    "print(\"Saved optimal parameters summary to output/tables/optimal_parameters_summary.csv\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 4: MODEL TRAINING AND EVALUATION\n",
    "# ==============================================\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate model performance and return metrics.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'F1_Score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'R2_Score': r2_score(y_test, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred),\n",
    "        'ROC_AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else np.nan\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Initialize results storage\n",
    "results = []\n",
    "mc_results = {name: {'train_r2': [], 'test_r2': [], 'rmse': []} for name in models.keys()}\n",
    "\n",
    "# Monte Carlo simulations\n",
    "for sim in range(N_SIMULATIONS):\n",
    "    print(f\"\\n🚀 Simulation {sim+1}/{N_SIMULATIONS}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED+sim)\n",
    "    \n",
    "    for name, details in models.items():\n",
    "        print(f\"  - Training {name}...\")\n",
    "        \n",
    "        # Create pipeline\n",
    "        pipeline = ImbPipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=RANDOM_SEED+sim)),\n",
    "            ('model', details['model'](**details['params']))\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            # Fit model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = evaluate_model(pipeline, X_test, y_test)\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Simulation': sim+1,\n",
    "                'Model': name,\n",
    "                **metrics\n",
    "            })\n",
    "            \n",
    "            # Cross validation for R2 score\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, \n",
    "                                     cv=CV_FOLDS, scoring='r2', n_jobs=N_JOBS)\n",
    "            \n",
    "            # Store MC results\n",
    "            mc_results[name]['train_r2'].append(np.mean(cv_scores))\n",
    "            mc_results[name]['test_r2'].append(metrics['R2_Score'])\n",
    "            mc_results[name]['rmse'].append(metrics['RMSE'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name} in simulation {sim+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# Save performance metrics to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('output/tables/performance_metrics.csv', index=False)\n",
    "print(\"\\nSaved performance metrics to output/tables/performance_metrics.csv\")\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 5: VISUALIZATIONS\n",
    "# ==============================================\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 5: VISUALIZATIONS (UPDATED)\n",
    "# ==============================================\n",
    "\n",
    "# Update the MC results storage to track all metrics\n",
    "mc_results = {name: {\n",
    "    'train_r2': [],\n",
    "    'test_r2': [],\n",
    "    'accuracy': [],\n",
    "    'f1_score': [],\n",
    "    'roc_auc': [],\n",
    "    'rmse': [],\n",
    "    'mae': []\n",
    "} for name in models.keys()}\n",
    "\n",
    "# Modify the simulation loop to store all metrics\n",
    "for sim in range(N_SIMULATIONS):\n",
    "    # ... [existing simulation code] ...\n",
    "    \n",
    "    for name, details in models.items():\n",
    "        # ... [existing training code] ...\n",
    "        \n",
    "        try:\n",
    "            # ... [existing fitting code] ...\n",
    "            \n",
    "            # Store all metrics\n",
    "            mc_results[name]['train_r2'].append(np.mean(cv_scores))\n",
    "            mc_results[name]['test_r2'].append(metrics['R2_Score'])\n",
    "            mc_results[name]['accuracy'].append(metrics['Accuracy'])\n",
    "            mc_results[name]['f1_score'].append(metrics['F1_Score'])\n",
    "            mc_results[name]['roc_auc'].append(metrics['ROC_AUC'])\n",
    "            mc_results[name]['rmse'].append(metrics['RMSE'])\n",
    "            mc_results[name]['mae'].append(metrics['MAE'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {name} in simulation {sim+1}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "# 1. Updated Training vs Testing Performance Across Simulations (All metrics)\n",
    "metrics_to_plot = ['accuracy', 'f1_score', 'roc_auc', 'rmse', 'mae']\n",
    "metric_names = ['Accuracy', 'F1 Score', 'ROC AUC', 'RMSE', 'MAE']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    for name in models.keys():\n",
    "        plt.plot(range(1, N_SIMULATIONS+1), mc_results[name][metric], \n",
    "                 label=name, alpha=0.8)\n",
    "    plt.xlabel('Simulation Number')\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.title(f'{metric_names[i]} Across Simulations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    if i == 0:  # Only show legend on first plot\n",
    "        plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/all_metrics_across_simulations.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Metric distribution comparison (Boxplot version)\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    metric_data = []\n",
    "    for name in models.keys():\n",
    "        metric_data.append(mc_results[name][metric])\n",
    "    \n",
    "    plt.boxplot(metric_data, labels=models.keys())\n",
    "    plt.title(f'{metric_names[i]} Distribution')\n",
    "    plt.ylabel(metric_names[i])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/metric_distribution_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Metric correlation heatmap\n",
    "corr_metrics = ['accuracy', 'f1_score', 'roc_auc', 'rmse', 'mae']\n",
    "corr_names = ['Accuracy', 'F1', 'AUC', 'RMSE', 'MAE']\n",
    "\n",
    "# Calculate correlations across all simulations\n",
    "all_metrics = []\n",
    "for name in models.keys():\n",
    "    for sim in range(N_SIMULATIONS):\n",
    "        metrics = {m: mc_results[name][m][sim] for m in corr_metrics}\n",
    "        metrics['Model'] = name\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "corr_df = pd.DataFrame(all_metrics)\n",
    "corr_matrix = corr_df[corr_metrics].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', \n",
    "            xticklabels=corr_names, yticklabels=corr_names,\n",
    "            vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Metric Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/metric_correlations.png')\n",
    "plt.close()\n",
    "\n",
    "# ... [keep all other existing visualization code] ...\n",
    "\n",
    "# 3. Model fitting diagram (Underfitting/Overfitting/Balanced)\n",
    "plt.figure(figsize=(8, 6))\n",
    "x = np.linspace(0, 1, 100)\n",
    "y_true = np.sin(2 * np.pi * x)\n",
    "\n",
    "# Underfit\n",
    "y_under = 0.5 * x + 0.2\n",
    "# Overfit\n",
    "y_over = np.sin(2 * np.pi * x) + 0.3 * np.cos(20 * np.pi * x)\n",
    "# Good fit\n",
    "y_good = np.sin(2 * np.pi * x) + np.random.normal(0, 0.1, len(x))\n",
    "\n",
    "plt.scatter(x, y_true + np.random.normal(0, 0.05, len(x)), \n",
    "            color='blue', alpha=0.3, label='Data points')\n",
    "plt.plot(x, y_under, 'r-', linewidth=2, label='Underfitting')\n",
    "plt.plot(x, y_over, 'g-', linewidth=2, label='Overfitting')\n",
    "plt.plot(x, y_good, 'k--', linewidth=2, label='Good fit')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Model Fitting Illustration')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/model_fitting_diagram.png')\n",
    "plt.close()\n",
    "\n",
    "# 4. WCT Predictions Statistics\n",
    "# Get predictions from last simulation\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=TEST_SIZE, stratify=y, random_state=RANDOM_SEED+N_SIMULATIONS)\n",
    "\n",
    "best_model = ImbPipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=RANDOM_SEED)),\n",
    "    ('model', XGBClassifier(**models['XGBoost']['params']))\n",
    "])\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('WCT Prediction Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/wct_confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Prediction distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "pd.Series(y_pred).value_counts().plot(kind='bar')\n",
    "plt.xlabel('WCT Prediction')\n",
    "plt.ylabel('Count')\n",
    "plt.title('WCT Prediction Distribution')\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'], rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/plots/wct_prediction_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# ==============================================\n",
    "# SECTION 6: FINAL OUTPUTS\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n🔥 All tasks completed successfully!\")\n",
    "print(\"Generated outputs:\")\n",
    "print(\"- Model hyperparameters table\")\n",
    "print(\"- Performance metrics CSV\")\n",
    "print(\"- Training vs testing performance plot\")\n",
    "print(\"- RMSE across simulations plot\")\n",
    "print(\"- Model fitting diagram\")\n",
    "print(\"- WCT prediction statistics plots\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined confusion matrices to output/plots/all_confusion_matrices.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set style for academic publication\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "\n",
    "def plot_all_confusion_matrices(y_true, y_preds, model_names):\n",
    "    \"\"\"\n",
    "    Generate a 2x2 grid of confusion matrices for four models\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): True labels (0 or 1)\n",
    "    y_preds (list of arrays): Predictions from each model\n",
    "    model_names (list): Names of the four models\n",
    "    \"\"\"\n",
    "    # Create figure and axes\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Confusion Matrices Comparison Across Models', y=1.02, fontsize=14)\n",
    "    \n",
    "    # Define colormaps for each model\n",
    "    cmaps = ['Blues', 'Oranges', 'Greens', 'Reds']\n",
    "    \n",
    "    # Plot each confusion matrix\n",
    "    for i, (pred, name, cmap) in enumerate(zip(y_preds, model_names, cmaps)):\n",
    "        ax = axs[i//2, i%2]\n",
    "        cm = confusion_matrix(y_true, pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
    "                   cbar=False, annot_kws={'size': 10})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        # Set titles and labels\n",
    "        ax.set_title(f'{name}\\nSens: {sensitivity:.3f}, Spec: {specificity:.3f}', \n",
    "                    pad=12, fontsize=11)\n",
    "        ax.set_xlabel('Predicted Label', labelpad=8)\n",
    "        ax.set_ylabel('True Label', labelpad=8)\n",
    "        ax.set_xticklabels(['Negative', 'Positive'])\n",
    "        ax.set_yticklabels(['Negative', 'Positive'], rotation=0)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/all_confusion_matrices.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved combined confusion matrices to output/plots/all_confusion_matrices.png\")\n",
    "\n",
    "# Example usage with mock data (replace with your actual predictions)\n",
    "y_true = np.random.randint(0, 2, 1000)  # True labels\n",
    "y_preds = [\n",
    "    np.random.binomial(1, 0.98, 1000),  # XGBoost predictions\n",
    "    np.random.binomial(1, 0.97, 1000),  # LightGBM predictions\n",
    "    np.random.binomial(1, 0.96, 1000),  # Random Forest predictions\n",
    "    np.random.binomial(1, 0.95, 1000)   # Gradient Boosting predictions\n",
    "]\n",
    "model_names = ['XGBoost', 'LightGBM', 'Random Forest', 'Gradient Boosting']\n",
    "\n",
    "plot_all_confusion_matrices(y_true, y_preds, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set style for academic publication\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['axes.titlesize'] = 12\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['xtick.labelsize'] = 9\n",
    "plt.rcParams['ytick.labelsize'] = 9\n",
    "\n",
    "def generate_model_analysis_plots(y_true, y_preds, feature_importances, model_names, features):\n",
    "    \"\"\"\n",
    "    Generate comprehensive model evaluation plots including:\n",
    "    - Confusion matrices for all models\n",
    "    - Feature importance comparison\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array): True labels (0 or 1)\n",
    "    y_preds (list of arrays): Predictions from each model\n",
    "    feature_importances (list of arrays): Feature importance scores\n",
    "    model_names (list): Names of the models\n",
    "    features (list): Feature names\n",
    "    \"\"\"\n",
    "    # Create figure with 2 rows (confusion matrices and feature importance)\n",
    "    fig = plt.figure(figsize=(14, 14))\n",
    "    gs = fig.add_gridspec(2, 1, height_ratios=[1, 1.2])\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Confusion Matrices (Top Row)\n",
    "    # --------------------------------------------------\n",
    "    axs_conf = fig.add_subplot(gs[0])\n",
    "    axs_conf.axis('off')  # Turn off main axis for subplots\n",
    "    \n",
    "    # Create 2x2 grid for confusion matrices\n",
    "    inner_gs_conf = gs[0].subgridspec(2, 2, wspace=0.15, hspace=0.25)\n",
    "    axs_conf = inner_gs_conf.subplots()\n",
    "    \n",
    "    # Define colormaps for each model\n",
    "    cmaps = ['Blues', 'Oranges', 'Greens', 'Reds']\n",
    "    \n",
    "    # Plot each confusion matrix\n",
    "    for i, (pred, name, cmap) in enumerate(zip(y_preds, model_names, cmaps)):\n",
    "        ax = axs_conf[i//2, i%2]\n",
    "        cm = confusion_matrix(y_true, pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, ax=ax,\n",
    "                   cbar=False, annot_kws={'size': 10})\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # Set titles and labels\n",
    "        ax.set_title(f'{name}\\nSens: {sensitivity:.3f}, Spec: {specificity:.3f}\\nAcc: {accuracy:.3f}', \n",
    "                    pad=12, fontsize=11)\n",
    "        ax.set_xlabel('Predicted Label', labelpad=8)\n",
    "        ax.set_ylabel('True Label', labelpad=8)\n",
    "        ax.set_xticklabels(['Negative', 'Positive'])\n",
    "        ax.set_yticklabels(['Negative', 'Positive'], rotation=0)\n",
    "        \n",
    "        # Add border\n",
    "        for _, spine in ax.spines.items():\n",
    "            spine.set_visible(True)\n",
    "            spine.set_linewidth(1.5)\n",
    "    \n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Feature Importance (Bottom Row)\n",
    "    # --------------------------------------------------\n",
    "    axs_feat = fig.add_subplot(gs[1])\n",
    "    axs_feat.axis('off')  # Turn off main axis for subplots\n",
    "    \n",
    "    # Create dataframe for feature importances\n",
    "    feat_df = pd.DataFrame(feature_importances, index=model_names, columns=features).T\n",
    "    feat_df = feat_df.sort_values(by=model_names[0], ascending=False)\n",
    "    \n",
    "    # Create 2x2 grid for feature importance\n",
    "    inner_gs_feat = gs[1].subgridspec(2, 2, wspace=0.15, hspace=0.3)\n",
    "    axs_feat = inner_gs_feat.subplots()\n",
    "    \n",
    "    # Plot feature importance for each model\n",
    "    for i, (name, cmap) in enumerate(zip(model_names, cmaps)):\n",
    "        ax = axs_feat[i//2, i%2]\n",
    "        sns.barplot(x=feat_df[name].values, y=feat_df.index, ax=ax, \n",
    "                   color=sns.color_palette(cmap)[2], saturation=0.8)\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_title(f'{name} Feature Importance', pad=12, fontsize=11)\n",
    "        ax.set_xlabel('Importance Score', labelpad=8)\n",
    "        ax.set_ylabel('')\n",
    "        ax.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add value labels for top 3 features\n",
    "        for j, (feature, val) in enumerate(zip(feat_df.index, feat_df[name])):\n",
    "            if j < 3:  # Only label top 3 features\n",
    "                ax.text(val + 0.01, j, f'{val:.3f}', \n",
    "                        va='center', fontsize=9)\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/full_model_analysis.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved comprehensive model analysis to output/plots/full_model_analysis.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved density plot to output/plots/metric_density.png\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "# Data from Table 5.2\n",
    "data = {\n",
    "    'Model': ['XGBoost', 'LightGBM', 'Random Forest', 'Gradient Boosting'],\n",
    "    'Avg Accuracy': [0.9998, 0.9997, 0.9995, 0.9993],\n",
    "    'F1 Score': [0.9998, 0.9996, 0.9994, 0.9992],\n",
    "    'ROC AUC': [0.9999, 0.9998, 0.9996, 0.9995],\n",
    "    'Max RMSE': [0.0071, 0.0122, 0.0122, 0.0158]\n",
    "}\n",
    "\n",
    "# Create density plot\n",
    "def plot_metric_density():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create density distributions for each metric\n",
    "    metrics = ['Avg Accuracy', 'F1 Score', 'ROC AUC', 'Max RMSE']\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "    \n",
    "    for metric, color in zip(metrics, colors):\n",
    "        values = data[metric]\n",
    "        \n",
    "        # Create density estimate\n",
    "        density = gaussian_kde(values)\n",
    "        xs = np.linspace(min(values)-0.0005, max(values)+0.0005, 200)\n",
    "        \n",
    "        # Plot density curve\n",
    "        ax.plot(xs, density(xs), color=color, label=metric)\n",
    "        \n",
    "        # Add rug plot\n",
    "        ax.scatter(values, [-0.0005]*len(values), color=color, marker='|', s=100)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlim(0.999, 1.0005)\n",
    "    ax.set_ylim(-0.005, 25)\n",
    "    ax.set_xlabel('Metric Value', fontsize=10)\n",
    "    ax.set_ylabel('Density', fontsize=10)\n",
    "    ax.set_title('Model Metric Density Distributions', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    # Add model labels at bottom\n",
    "    for i, model in enumerate(data['Model']):\n",
    "        ax.text(0.9991 + i*0.0003, -0.003, model, \n",
    "                rotation=45, fontsize=8, ha='left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/metric_density.png')\n",
    "    plt.close()\n",
    "    print(\"Saved density plot to output/plots/metric_density.png\")\n",
    "\n",
    "# Generate plot\n",
    "plot_metric_density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Temp\\ipykernel_22388\\2532597495.py:66: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Temp\\ipykernel_22388\\2532597495.py:67: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  plt.savefig('statistical_analysis/rolling_stats.png')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical line graphs generated and saved to 'statistical_analysis' directory\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "\n",
    "# Load and prepare the data\n",
    "df = pd.read_csv('merged_ecg_data_cleaned.csv')\n",
    "\n",
    "# Convert string ranges to numeric (take first value)\n",
    "def convert_freq_range(x):\n",
    "    if isinstance(x, str) and '-' in x and 'Hz' in x:\n",
    "        return float(x.split('-')[0])\n",
    "    return x\n",
    "\n",
    "df['bandwidth'] = df['bandwidth'].apply(convert_freq_range)\n",
    "df['filtering'] = df['filtering'].apply(convert_freq_range)\n",
    "\n",
    "# Ensure numeric types\n",
    "numeric_features = ['rr_interval', 'p_onset', 'p_end', 'qrs_onset', \n",
    "                   'qrs_end', 't_end', 'p_axis', 'qrs_axis', \n",
    "                   't_axis', 'qrs_duration']\n",
    "for feat in numeric_features:\n",
    "    df[feat] = pd.to_numeric(df[feat], errors='coerce')\n",
    "\n",
    "# Handle target variable\n",
    "target = 'wct_label_encoded' if 'wct_label_encoded' in df.columns else 'wct_label'\n",
    "if not np.issubdtype(df[target].dtype, np.number):\n",
    "    df[target] = df[target].astype('category').cat.codes\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('statistical_analysis', exist_ok=True)\n",
    "\n",
    "# 1. Temporal Feature Trends (Line Graphs)\n",
    "temporal_features = ['rr_interval', 'p_onset', 'qrs_onset', 't_end']\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for i, feat in enumerate(temporal_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.lineplot(data=df, x=df.index, y=feat, hue=target, \n",
    "                estimator='mean', errorbar=('ci', 95))\n",
    "    plt.title(f'Trend of {feat}')\n",
    "    plt.xlabel('Time Sequence')\n",
    "    plt.ylabel(feat)\n",
    "    plt.legend(title='WCT Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistical_analysis/temporal_trends.png')\n",
    "plt.close()\n",
    "\n",
    "# 2. Statistical Distribution Over Time\n",
    "stats_to_plot = ['mean', 'std', 'skew']\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, stat in enumerate(stats_to_plot, 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    df_rolling = df[numeric_features].rolling(100).agg(stat)\n",
    "    for feat in ['rr_interval', 'qrs_duration']:  # Key features\n",
    "        sns.lineplot(data=df_rolling, x=df_rolling.index, y=feat)\n",
    "    plt.title(f'Rolling {stat} of Key Features')\n",
    "    plt.xlabel('Time Sequence')\n",
    "    plt.ylabel(stat)\n",
    "    plt.legend(['RR Interval', 'QRS Duration'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistical_analysis/rolling_stats.png')\n",
    "plt.close()\n",
    "\n",
    "# 3. Statistical Significance Over Time\n",
    "significant_features = []\n",
    "window_size = 500  # Analyze in windows of 500 samples\n",
    "\n",
    "for feat in numeric_features:\n",
    "    p_values = []\n",
    "    for i in range(0, len(df), window_size):\n",
    "        window = df.iloc[i:i+window_size]\n",
    "        if len(window[target].unique()) == 2:  # Both classes present\n",
    "            group0 = window[window[target] == 0][feat].dropna()\n",
    "            group1 = window[window[target] == 1][feat].dropna()\n",
    "            if len(group0) > 10 and len(group1) > 10:\n",
    "                _, p = stats.mannwhitneyu(group0, group1)\n",
    "                p_values.append(p)\n",
    "    \n",
    "    if p_values and np.mean(p_values) < 0.05:\n",
    "        significant_features.append(feat)\n",
    "\n",
    "# 4. Plot Most Significant Features\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, feat in enumerate(significant_features[:3], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.lineplot(data=df, x=df.index, y=feat, hue=target,\n",
    "                estimator='median', errorbar=None)\n",
    "    plt.title(f'{feat} by WCT Status')\n",
    "    plt.xlabel('Time Sequence')\n",
    "    plt.ylabel(feat)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistical_analysis/significant_features_trends.png')\n",
    "plt.close()\n",
    "\n",
    "# 5. Statistical Metric Trends\n",
    "metrics = ['mean', 'std', 'median']\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    df_grouped = df.groupby(target)[numeric_features].agg(metric).T\n",
    "    df_grouped.plot(marker='o', ax=plt.gca())\n",
    "    plt.title(f'{metric.capitalize()} Values by WCT Status')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='WCT Status')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('statistical_analysis/metric_trends.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Statistical line graphs generated and saved to 'statistical_analysis' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged_ecg_data_cleaned.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (800035, 12), Features: ['bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end', 'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', 't_axis', 'qrs_duration']\n",
      "\n",
      "Training XGBoost model for interpretation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating permutation importance...\n",
      "✅ Saved ECG saliency map to output/plots/ecg_saliency_map.png\n",
      "\n",
      "Generating SHAP explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Temp\\ipykernel_22388\\4152921214.py:185: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1437: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "C:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Temp\\ipykernel_22388\\4152921214.py:208: FutureWarning: The NumPy global RNG was seeded by calling `np.random.seed`. In a future version this function will no longer use the global RNG. Pass `rng` explicitly to opt-in to the new behaviour and silence this warning.\n",
      "  shap.summary_plot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved SHAP visualizations to output/plots/\n",
      "✅ Saved ECG group importance to output/plots/ecg_group_importance.png\n",
      "\n",
      "🎉 ECG Saliency Analysis Complete!\n",
      "Generated the following clinical visualizations:\n",
      "1. output/plots/ecg_saliency_map.png - Main permutation importance\n",
      "2. output/plots/ecg_shap_beeswarm.png - Detailed SHAP values\n",
      "3. output/plots/ecg_shap_importance.png - SHAP importance ranking\n",
      "4. output/plots/ecg_group_importance.png - Clinical feature groups\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# COMPLETE SALIENCY VISUALIZATION FOR ECG WCT DETECTION\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import os\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "os.makedirs('output/tables', exist_ok=True)\n",
    "\n",
    "# ==============================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ==============================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the ECG dataset\"\"\"\n",
    "    print(\"Loading merged_ecg_data_cleaned.csv...\")\n",
    "    try:\n",
    "        df = pd.read_csv('merged_ecg_data_cleaned.csv')\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Could not find 'merged_ecg_data_cleaned.csv'. Please ensure the file exists in the current directory.\")\n",
    "    \n",
    "    # Select features and target\n",
    "    features = ['bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end', \n",
    "               'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', \n",
    "               't_axis', 'qrs_duration']\n",
    "    target = 'wct_label_encoded' if 'wct_label_encoded' in df.columns else 'wct_label'\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in features:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Encode target if needed\n",
    "    if not np.issubdtype(df[target].dtype, np.number):\n",
    "        df[target] = df[target].astype('category').cat.codes\n",
    "    \n",
    "    X = df[features].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}, Features: {features}\")\n",
    "    return X, y, features\n",
    "\n",
    "X, y, features = load_and_preprocess_data()\n",
    "\n",
    "# ==============================================\n",
    "# 2. MODEL TRAINING\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nTraining XGBoost model for interpretation...\")\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "model.fit(X, y)\n",
    "\n",
    "# ==============================================\n",
    "# 3. ENHANCED SALIENCY VISUALIZATIONS\n",
    "# ==============================================\n",
    "\n",
    "def plot_ecg_saliency(model, X, y, features, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate beautiful saliency visualizations for ECG features\n",
    "    \"\"\"\n",
    "    # Permutation Importance\n",
    "    print(\"Calculating permutation importance...\")\n",
    "    result = permutation_importance(\n",
    "        model, X[:n_samples], y[:n_samples],\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "    sorted_features = np.array(features)[sorted_idx]\n",
    "    sorted_importance = result.importances[sorted_idx]\n",
    "    \n",
    "    # Create ECG-appropriate color gradient\n",
    "    ecg_colors = [\"#FF6B6B\", \"#FFA3A3\", \"#FFD3B6\", \"#DCE2C8\", \"#A5D8D6\", \"#74B3CE\"]\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"ecg_cmap\", ecg_colors)\n",
    "    \n",
    "    # Create figure with ECG styling\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    plt.style.use('default')  # Using default style\n",
    "    \n",
    "    # Create horizontal violin plot\n",
    "    parts = ax.violinplot(\n",
    "        sorted_importance.T,\n",
    "        vert=False,\n",
    "        showmeans=True,\n",
    "        showextrema=True,\n",
    "        widths=0.7\n",
    "    )\n",
    "    \n",
    "    # Color each violin by importance\n",
    "    for pc, color in zip(parts['bodies'], np.linspace(0, 1, len(sorted_features))):\n",
    "        pc.set_facecolor(cmap(color))\n",
    "        pc.set_edgecolor('#2d3436')\n",
    "        pc.set_alpha(0.9)\n",
    "    \n",
    "    # ECG-specific styling\n",
    "    ax.set_yticks(range(1, len(sorted_features)+1))\n",
    "    ax.set_yticklabels(sorted_features, fontsize=10, fontfamily='serif')\n",
    "    ax.set_title(\"ECG Feature Importance for WCT Detection\\n(Permutation Importance Analysis)\", \n",
    "                fontsize=16, pad=20, fontweight='bold', fontfamily='serif')\n",
    "    ax.set_xlabel(\"Mean Accuracy Decrease When Permuted\", \n",
    "                 fontsize=12, fontfamily='serif')\n",
    "    ax.set_ylabel(\"ECG Features\", fontsize=12, fontfamily='serif')\n",
    "    \n",
    "    # Add clinical context annotation\n",
    "    ax.annotate(\n",
    "        \"Higher values indicate features more critical\\nfor Wide Complex Tachycardia detection\",\n",
    "        xy=(0.7, 0.9), xycoords='axes fraction',\n",
    "        fontsize=10, fontfamily='serif',\n",
    "        bbox=dict(boxstyle=\"round\", alpha=0.1, facecolor=\"white\")\n",
    "    )\n",
    "    \n",
    "    # Final styling\n",
    "    ax.grid(True, linestyle='--', alpha=0.2)\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Add professional colorbar - FIXED with proper mappable\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=0, vmax=len(sorted_features)))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(sm, ax=ax, pad=0.02)\n",
    "    cbar.set_label('Feature Importance Rank', \n",
    "                  rotation=270, \n",
    "                  labelpad=20,\n",
    "                  fontfamily='serif')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/ecg_saliency_map.png', \n",
    "               dpi=300, \n",
    "               bbox_inches='tight',\n",
    "               transparent=True)\n",
    "    plt.close()\n",
    "    \n",
    "    return result, sorted_features\n",
    "\n",
    "# Generate the main saliency visualization\n",
    "perm_result, important_features = plot_ecg_saliency(model, X, y, features)\n",
    "print(\"✅ Saved ECG saliency map to output/plots/ecg_saliency_map.png\")\n",
    "\n",
    "# ==============================================\n",
    "# 4. SHAP VALUE ANALYSIS (DETAILED FEATURE IMPACT)\n",
    "# ==============================================\n",
    "\n",
    "def plot_shap_analysis(model, X, features, n_samples=500):\n",
    "    \"\"\"Generate SHAP value visualizations\"\"\"\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        \n",
    "        # Transform data through pipeline\n",
    "        X_transformed = model.named_steps['scaler'].transform(X[:n_samples])\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        shap_values = explainer.shap_values(X_transformed)\n",
    "        \n",
    "        # Create clinical color scheme (red=positive, blue=negative)\n",
    "        shap_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "            \"shap_clinical\", [\"#1e90ff\", \"#f1f2f6\", \"#ff4757\"])\n",
    "        \n",
    "        # Beeswarm plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(\n",
    "            shap_values, \n",
    "            X_transformed, \n",
    "            feature_names=features,\n",
    "            plot_type=\"dot\",\n",
    "            color=shap_cmap,\n",
    "            show=False,\n",
    "            plot_size=(12, 8)\n",
    "        )\n",
    "        \n",
    "        # Clinical styling\n",
    "        plt.title(\"SHAP Feature Impact on WCT Predictions\\n(Red=Higher Risk, Blue=Lower Risk)\", \n",
    "                 fontsize=14, pad=20, fontweight='bold', fontfamily='serif')\n",
    "        plt.gcf().set_facecolor('#f8f9fa')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output/plots/ecg_shap_beeswarm.png', \n",
    "                   dpi=300, \n",
    "                   bbox_inches='tight',\n",
    "                   transparent=True)\n",
    "        plt.close()\n",
    "        \n",
    "        # Feature importance plot\n",
    "        plt.figure()\n",
    "        shap.summary_plot(\n",
    "            shap_values, \n",
    "            X_transformed, \n",
    "            feature_names=features,\n",
    "            plot_type=\"bar\",\n",
    "            show=False,\n",
    "            plot_size=(10, 6)\n",
    "        )\n",
    "        plt.title(\"SHAP Feature Importance Ranking\", \n",
    "                 fontsize=14, pad=20, fontweight='bold', fontfamily='serif')\n",
    "        plt.gcf().set_facecolor('#f8f9fa')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output/plots/ecg_shap_importance.png', \n",
    "                   dpi=300, \n",
    "                   bbox_inches='tight',\n",
    "                   transparent=True)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not generate SHAP plots: {str(e)}\")\n",
    "        print(\"You may need to install SHAP: pip install shap\")\n",
    "\n",
    "# Generate SHAP plots\n",
    "plot_shap_analysis(model, X, features)\n",
    "print(\"✅ Saved SHAP visualizations to output/plots/\")\n",
    "\n",
    "# ==============================================\n",
    "# 5. ECG FEATURE GROUP ANALYSIS\n",
    "# ==============================================\n",
    "\n",
    "def plot_ecg_feature_groups(importance_result, features):\n",
    "    \"\"\"Visualize importance by clinical ECG feature groups\"\"\"\n",
    "    # Define clinically meaningful groups\n",
    "    ecg_groups = {\n",
    "        'Timing Intervals': ['rr_interval', 'p_onset', 'p_end', \n",
    "                            'qrs_onset', 'qrs_end', 't_end', 'qrs_duration'],\n",
    "        'Electrical Axes': ['p_axis', 'qrs_axis', 't_axis'],\n",
    "        'Signal Quality': ['bandwidth', 'filtering']\n",
    "    }\n",
    "    \n",
    "    # Calculate group importances\n",
    "    group_data = []\n",
    "    for group_name, group_features in ecg_groups.items():\n",
    "        group_mask = [f in group_features for f in features]\n",
    "        group_mean = np.mean(importance_result.importances_mean[group_mask])\n",
    "        group_std = np.mean(importance_result.importances_std[group_mask])\n",
    "        group_data.append({\n",
    "            'Group': group_name,\n",
    "            'Importance': group_mean,\n",
    "            'Std': group_std\n",
    "        })\n",
    "    \n",
    "    group_df = pd.DataFrame(group_data).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = plt.cm.PuBu(np.linspace(0.3, 0.9, len(group_df)))\n",
    "    \n",
    "    ax.barh(\n",
    "        group_df['Group'],\n",
    "        group_df['Importance'],\n",
    "        xerr=group_df['Std'],\n",
    "        color=colors,\n",
    "        alpha=0.8,\n",
    "        capsize=5\n",
    "    )\n",
    "    \n",
    "    # Clinical ECG styling\n",
    "    ax.set_title(\"WCT Detection Importance by ECG Feature Category\", \n",
    "                fontsize=16, pad=20, fontweight='bold', fontfamily='serif')\n",
    "    ax.set_xlabel(\"Mean Permutation Importance\", fontsize=12, fontfamily='serif')\n",
    "    ax.set_ylabel(\"ECG Feature Categories\", fontsize=12, fontfamily='serif')\n",
    "    ax.grid(True, linestyle='--', alpha=0.2)\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/ecg_group_importance.png', \n",
    "               dpi=300, \n",
    "               bbox_inches='tight',\n",
    "               transparent=True)\n",
    "    plt.close()\n",
    "\n",
    "# Generate grouped importance plot\n",
    "plot_ecg_feature_groups(perm_result, features)\n",
    "print(\"✅ Saved ECG group importance to output/plots/ecg_group_importance.png\")\n",
    "\n",
    "# ==============================================\n",
    "# FINAL OUTPUT\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n🎉 ECG Saliency Analysis Complete!\")\n",
    "print(\"Generated the following clinical visualizations:\")\n",
    "print(\"1. output/plots/ecg_saliency_map.png - Main permutation importance\")\n",
    "print(\"2. output/plots/ecg_shap_beeswarm.png - Detailed SHAP values\")\n",
    "print(\"3. output/plots/ecg_shap_importance.png - SHAP importance ranking\")\n",
    "print(\"4. output/plots/ecg_group_importance.png - Clinical feature groups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading merged_ecg_data_cleaned.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1241: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (800035, 12), Features: ['bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end', 'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', 't_axis', 'qrs_duration']\n",
      "\n",
      "Training XGBoost model for interpretation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating permutation importance...\n",
      "✅ Saved ECG saliency map to output/plots/ecg_saliency_map.png\n",
      "\n",
      "Generating SHAP explanations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MR. VASKAR CHAKMA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1437: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved SHAP visualizations to output/plots/\n",
      "✅ Saved ECG group importance to output/plots/ecg_group_importance.png\n",
      "\n",
      "🎉 ECG Saliency Analysis Complete!\n",
      "Generated the following visualizations:\n",
      "1. output/plots/ecg_saliency_map.png - Feature importance\n",
      "2. output/plots/ecg_shap_beeswarm.png - SHAP values\n",
      "3. output/plots/ecg_group_importance.png - Feature groups\n"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# ECG SALIENCY VISUALIZATION - FINAL WORKING VERSION\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import os\n",
    "\n",
    "# Set up directories\n",
    "os.makedirs('output/plots', exist_ok=True)\n",
    "os.makedirs('output/tables', exist_ok=True)\n",
    "\n",
    "# ==============================================\n",
    "# 1. DATA LOADING AND PREPROCESSING\n",
    "# ==============================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the ECG dataset\"\"\"\n",
    "    print(\"Loading merged_ecg_data_cleaned.csv...\")\n",
    "    try:\n",
    "        df = pd.read_csv('merged_ecg_data_cleaned.csv')\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Could not find 'merged_ecg_data_cleaned.csv'\")\n",
    "    \n",
    "    # Select features and target\n",
    "    features = ['bandwidth', 'filtering', 'rr_interval', 'p_onset', 'p_end', \n",
    "               'qrs_onset', 'qrs_end', 't_end', 'p_axis', 'qrs_axis', \n",
    "               't_axis', 'qrs_duration']\n",
    "    target = 'wct_label_encoded' if 'wct_label_encoded' in df.columns else 'wct_label'\n",
    "    \n",
    "    # Handle missing values\n",
    "    for col in features:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # Encode target if needed\n",
    "    if not np.issubdtype(df[target].dtype, np.number):\n",
    "        df[target] = df[target].astype('category').cat.codes\n",
    "    \n",
    "    X = df[features].values\n",
    "    y = df[target].values\n",
    "    \n",
    "    print(f\"Data shape: {X.shape}, Features: {features}\")\n",
    "    return X, y, features\n",
    "\n",
    "X, y, features = load_and_preprocess_data()\n",
    "\n",
    "# ==============================================\n",
    "# 2. MODEL TRAINING\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\nTraining XGBoost model for interpretation...\")\n",
    "model = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "model.fit(X, y)\n",
    "\n",
    "# ==============================================\n",
    "# 3. ENHANCED SALIENCY VISUALIZATIONS\n",
    "# ==============================================\n",
    "\n",
    "def plot_ecg_saliency(model, X, y, features, n_samples=1000):\n",
    "    \"\"\"Generate beautiful saliency visualizations\"\"\"\n",
    "    # Permutation Importance\n",
    "    print(\"Calculating permutation importance...\")\n",
    "    result = permutation_importance(\n",
    "        model, X[:n_samples], y[:n_samples],\n",
    "        n_repeats=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_idx = result.importances_mean.argsort()[::-1]\n",
    "    sorted_features = np.array(features)[sorted_idx]\n",
    "    sorted_importance = result.importances[sorted_idx]\n",
    "    \n",
    "    # Create figure with modern style\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create boxplot with enhanced styling\n",
    "    boxprops = dict(linestyle='-', linewidth=1.5, color='darkblue')\n",
    "    whiskerprops = dict(linestyle='--', linewidth=1, color='black')\n",
    "    medianprops = dict(linestyle='-', linewidth=2, color='firebrick')\n",
    "    \n",
    "    bp = plt.boxplot(\n",
    "        sorted_importance.T,\n",
    "        vert=False,\n",
    "        patch_artist=True,\n",
    "        boxprops=boxprops,\n",
    "        whiskerprops=whiskerprops,\n",
    "        medianprops=medianprops\n",
    "    )\n",
    "    \n",
    "    # Color boxes with clinical color scheme\n",
    "    clinical_colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', \n",
    "                      '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
    "    for i, box in enumerate(bp['boxes']):\n",
    "        box.set_facecolor(clinical_colors[i % len(clinical_colors)])\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    # Add feature names\n",
    "    plt.yticks(range(1, len(sorted_features)+1), sorted_features, fontsize=10)\n",
    "    \n",
    "    # Add titles and labels\n",
    "    plt.title('ECG Feature Importance for WCT Detection', \n",
    "             fontsize=14, pad=20, fontweight='bold')\n",
    "    plt.xlabel('Mean Accuracy Decrease', fontsize=12)\n",
    "    plt.ylabel('ECG Features', fontsize=12)\n",
    "    \n",
    "    # Add grid\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/ecg_saliency_map.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return result, sorted_features\n",
    "\n",
    "# Generate the main saliency visualization\n",
    "perm_result, important_features = plot_ecg_saliency(model, X, y, features)\n",
    "print(\"✅ Saved ECG saliency map to output/plots/ecg_saliency_map.png\")\n",
    "\n",
    "# ==============================================\n",
    "# 4. SHAP VALUE ANALYSIS\n",
    "# ==============================================\n",
    "\n",
    "def plot_shap_analysis(model, X, features, n_samples=500):\n",
    "    \"\"\"Generate SHAP value visualizations\"\"\"\n",
    "    print(\"\\nGenerating SHAP explanations...\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare SHAP explainer\n",
    "        explainer = shap.TreeExplainer(model.named_steps['model'])\n",
    "        X_transformed = model.named_steps['scaler'].transform(X[:n_samples])\n",
    "        shap_values = explainer.shap_values(X_transformed)\n",
    "        \n",
    "        # Beeswarm plot with clinical colors\n",
    "        plt.figure()\n",
    "        shap.summary_plot(\n",
    "            shap_values, \n",
    "            X_transformed, \n",
    "            feature_names=features,\n",
    "            plot_type=\"dot\",\n",
    "            show=False,\n",
    "            plot_size=(12, 8),\n",
    "            cmap=plt.get_cmap('coolwarm')\n",
    "        )\n",
    "        plt.title(\"SHAP Feature Impact on WCT Predictions\", \n",
    "                 fontsize=14, pad=20, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output/plots/ecg_shap_beeswarm.png', dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not generate SHAP plots: {str(e)}\")\n",
    "\n",
    "# Generate SHAP plots\n",
    "plot_shap_analysis(model, X, features)\n",
    "print(\"✅ Saved SHAP visualizations to output/plots/\")\n",
    "\n",
    "# ==============================================\n",
    "# 5. ECG FEATURE GROUP ANALYSIS\n",
    "# ==============================================\n",
    "\n",
    "def plot_ecg_feature_groups(importance_result, features):\n",
    "    \"\"\"Visualize importance by clinical ECG feature groups\"\"\"\n",
    "    ecg_groups = {\n",
    "        'Timing Intervals': ['rr_interval', 'p_onset', 'p_end', 'qrs_onset', 'qrs_end', 't_end', 'qrs_duration'],\n",
    "        'Electrical Axes': ['p_axis', 'qrs_axis', 't_axis'],\n",
    "        'Signal Quality': ['bandwidth', 'filtering']\n",
    "    }\n",
    "    \n",
    "    # Calculate group importances\n",
    "    group_data = []\n",
    "    for group_name, group_features in ecg_groups.items():\n",
    "        group_mask = [f in group_features for f in features]\n",
    "        group_mean = np.mean(importance_result.importances_mean[group_mask])\n",
    "        group_data.append({'Group': group_name, 'Importance': group_mean})\n",
    "    \n",
    "    group_df = pd.DataFrame(group_data).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Create plot with clinical styling\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.barh(\n",
    "        group_df['Group'],\n",
    "        group_df['Importance'],\n",
    "        color=['#1f77b4', '#ff7f0e', '#2ca02c']  # Clinical color scheme\n",
    "    )\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width*1.02, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.3f}',\n",
    "                va='center')\n",
    "    \n",
    "    plt.title('WCT Detection Importance by ECG Feature Category', \n",
    "             fontsize=14, pad=20, fontweight='bold')\n",
    "    plt.xlabel('Mean Permutation Importance')\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/plots/ecg_group_importance.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# Generate grouped importance plot\n",
    "plot_ecg_feature_groups(perm_result, features)\n",
    "print(\"✅ Saved ECG group importance to output/plots/ecg_group_importance.png\")\n",
    "\n",
    "# ==============================================\n",
    "# FINAL OUTPUT\n",
    "# ==============================================\n",
    "\n",
    "print(\"\\n🎉 ECG Saliency Analysis Complete!\")\n",
    "print(\"Generated the following visualizations:\")\n",
    "print(\"1. output/plots/ecg_saliency_map.png - Feature importance\")\n",
    "print(\"2. output/plots/ecg_shap_beeswarm.png - SHAP values\")\n",
    "print(\"3. output/plots/ecg_group_importance.png - Feature groups\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
